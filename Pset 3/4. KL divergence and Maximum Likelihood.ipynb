{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KL divergence and Maximum Likelihood\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prove $\\forall P, Q$, $KL(P||Q) \\geq 0$\n",
    "\n",
    "$KL(P||Q) = E[\\log \\frac{P(x)}{Q(x)}]$\n",
    "\n",
    "( from Jensen's inequality )\n",
    "\n",
    "$\\qquad\\qquad \\geq \\log E[\\frac{P(x)}{Q(x)}]$\n",
    "\n",
    "( Also, we know that $\\forall x, \\log E[\\frac{P(x)}{Q(x)}] \\geq 0$ )\n",
    "\n",
    "$\\qquad\\qquad \\geq 0$\n",
    "\n",
    "2. Prove $KL(P||Q) = 0$ if and only if $P = Q$\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "$KL(P||Q) = 0 = \\Sigma_x P(x) \\log \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "( Since $\\forall x$, $P(x) > 0$ )\n",
    "\n",
    "$\\qquad \\qquad \\rightarrow \\forall x, \\log \\frac{P(x)}{Q(x)} = 0$\n",
    "\n",
    "$\\qquad \\qquad \\rightarrow \\forall x, \\frac{P(x)}{Q(x)} = 1$\n",
    "\n",
    "Therfore, $P(x) = Q(x)$\n",
    "\n",
    "$\\Leftarrow$\n",
    "\n",
    "Imply that $\\Sigma_x P(x) \\log 1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "$KL\\big[P(X, Y)||Q(X, Y)\\big] = \\Sigma_x \\Sigma_y P(x, y) \\log \\frac{P(x,y)}{Q(x,y)} $\n",
    "\n",
    "$\\qquad\\qquad = \\Sigma_x \\Sigma_y P(x, y) \\log (\\frac{P(y|x)}{Q(y|x)} \\cdot \\frac{P(x)}{Q(x)})$\n",
    "\n",
    "$\\qquad\\qquad = \\Sigma_x \\Sigma_y P(x, y)\\log\\frac{P(x)}{Q(x)} + \\Sigma_x \\Sigma_y P(x, y) \\log\\frac{P(y|x)}{Q(y|x)}$\n",
    "\n",
    "$\\qquad\\qquad = \\Sigma_x P(x) \\log\\frac{P(x)}{Q(x)} + \\Sigma_x P(x) \\Sigma_y P(y|x) \\log\\frac{P(y|x)}{Q(y|x)}$\n",
    "\n",
    "$\\qquad\\qquad = KL\\big[P(X)||Q(X)\\big] + KL\\big[P(Y | X)||Q(Y | X)\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "$KL\\big[\\hat{P}(X) || P_\\theta(X)\\big] = \\Sigma_x \\hat{P}(x) \\log \\frac{\\hat{P}(x)}{P_\\theta(x)}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\quad = \\Sigma_x \\frac{1}{m}\\Sigma_{i=1}^{m}1\\{x^{(i)} = x\\}\n",
    "\\log \\frac{\\frac{1}{m}\\Sigma_{i=1}^{m}1\\{x^{(i)} = x\\}}{P_\\theta(x^{(i)})}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\quad = \\frac{1}{m}\\Sigma_{i=1}^{m} \\log \\frac{1}{P_\\theta(x^{(i)})}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\quad = -\\frac{1}{m}\\Sigma_{i=1}^{m} \\log P_\\theta(x^{(i)})$\n",
    "\n",
    "Therefore, maximium likelihood estimating is same as minimizing $KL\\big[\\hat{P}(X) || P_\\theta(X)\\big]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
